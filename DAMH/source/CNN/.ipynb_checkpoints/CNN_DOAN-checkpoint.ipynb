{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import thư viện sử dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, time, json\n",
    "from string import punctuation\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,GlobalMaxPooling1D,LeakyReLU,Dense, Dropout,Conv1D, SpatialDropout1D,Reshape,Concatenate, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from collections import defaultdict\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "import time\n",
    "import sklearn\n",
    "import random \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import ntpath\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x3 data clean\n",
    "from pathlib import Path\n",
    "data_total = []\n",
    "label_total = []\n",
    "# clean_js path\n",
    "input_clean_dir = Path('H:/DoAnTotNghiep/DAMH/data_crawl/sontdc_data_js/clean_js/res')\n",
    "data_clean_files = input_clean_dir.glob('*')\n",
    "for file in data_clean_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read().replace('\\x00', '').lower()\n",
    "        data_total.append(data)\n",
    "        data_total.append(data)\n",
    "        label_total.append([1, 0])\n",
    "        label_total.append([1, 0])\n",
    "# malware_js path\n",
    "input_malware_dir = Path('H:/DoAnTotNghiep/DAMH/data_crawl/sontdc_data_js/virus_js/res')\n",
    "data_malware_files = input_malware_dir.glob('*')\n",
    "for file in data_malware_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read().replace('\\x00', '').lower()\n",
    "        data_total.append(data)\n",
    "        label_total.append([0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5222"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4699 523 4699 523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5152"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split train & test\n",
    "text_train, text_test, y_train, y_test = train_test_split(data_total, label_total, test_size=0.1, random_state=RANDOM_STATE)\n",
    "print(len(text_train), len(text_test), len(y_train), len(y_test))\n",
    "#num_words = 10000\n",
    "# Tokenize and transform to integer index\n",
    "tokenizer = Tokenizer(oov_token='UNK',filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to unk it word > num_words\n",
    "# mặc định thứ tự các từ đã sắp xếp theo tuần suất nên có thể loại các từ vị trí cao\n",
    "#tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= num_words}\n",
    "#tokenizer.word_index[tokenizer.oov_token] = num_words + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_test = tokenizer.texts_to_sequences(text_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "maxlen = max(len(x) for x in X_train) # longest text in train set\n",
    "\n",
    "MaxSize = 20000 # thay the maxlen vi qua lon\n",
    "# Add pading to ensure all vectors have same dimensionality\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=MaxSize) #thêm padding vào đằng sau (post) cho đến đủ chiều dài maxlen\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=MaxSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4699\n",
      "523\n"
     ]
    }
   ],
   "source": [
    "y_train=np.asarray(y_train)\n",
    "print(len(y_train))\n",
    "y_test=np.asarray(y_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxSize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[324]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index2 in range(len(X_train)):\n",
    "    if len(X_train[index2]) > MaxSize:\n",
    "        del X_train[index2][(MaxSize):] \n",
    "for index3 in range(len(X_test)):\n",
    "    if len(X_test[index3]) > MaxSize:\n",
    "        del X_test[index3][(MaxSize):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=MaxSize)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=MaxSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5153"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mô hình mạng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20000, 100)        515400    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 19996, 128)        64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 580,840\n",
      "Trainable params: 580,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define CNN architecture\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size + 1, embedding_dim, input_length=MaxSize))\n",
    "model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(10, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chạy huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 4699 samples, validate on 523 samples\n",
      "Epoch 1/15\n",
      "4699/4699 [==============================] - 626s 133ms/step - loss: 0.3581 - accuracy: 0.8434 - val_loss: 0.1785 - val_accuracy: 0.9312\n",
      "Epoch 2/15\n",
      "4699/4699 [==============================] - 642s 137ms/step - loss: 0.1188 - accuracy: 0.9596 - val_loss: 0.1147 - val_accuracy: 0.9627\n",
      "Epoch 3/15\n",
      "4699/4699 [==============================] - 624s 133ms/step - loss: 0.0663 - accuracy: 0.9781 - val_loss: 0.1136 - val_accuracy: 0.9627\n",
      "Epoch 4/15\n",
      "4699/4699 [==============================] - 630s 134ms/step - loss: 0.0491 - accuracy: 0.9836 - val_loss: 0.1027 - val_accuracy: 0.9694\n",
      "Epoch 5/15\n",
      "4699/4699 [==============================] - 621s 132ms/step - loss: 0.0361 - accuracy: 0.9895 - val_loss: 0.1286 - val_accuracy: 0.9694\n",
      "Epoch 6/15\n",
      "4699/4699 [==============================] - 670s 143ms/step - loss: 0.0355 - accuracy: 0.9894 - val_loss: 0.1369 - val_accuracy: 0.9732\n",
      "Epoch 7/15\n",
      "4699/4699 [==============================] - 642s 137ms/step - loss: 0.0294 - accuracy: 0.9922 - val_loss: 0.1254 - val_accuracy: 0.9723\n",
      "Epoch 8/15\n",
      "4699/4699 [==============================] - 636s 135ms/step - loss: 0.0338 - accuracy: 0.9913 - val_loss: 0.1455 - val_accuracy: 0.9656\n",
      "Epoch 9/15\n",
      "4699/4699 [==============================] - 633s 135ms/step - loss: 0.0221 - accuracy: 0.9908 - val_loss: 0.1434 - val_accuracy: 0.9732\n",
      "Epoch 10/15\n",
      "4699/4699 [==============================] - 613s 131ms/step - loss: 0.0207 - accuracy: 0.9928 - val_loss: 0.1764 - val_accuracy: 0.9637\n",
      "Epoch 11/15\n",
      "4699/4699 [==============================] - 637s 136ms/step - loss: 0.0155 - accuracy: 0.9924 - val_loss: 0.1543 - val_accuracy: 0.9713\n",
      "Epoch 12/15\n",
      "4699/4699 [==============================] - 644s 137ms/step - loss: 0.0211 - accuracy: 0.9915 - val_loss: 0.1461 - val_accuracy: 0.9713\n",
      "Epoch 13/15\n",
      "4699/4699 [==============================] - 610s 130ms/step - loss: 0.0178 - accuracy: 0.9922 - val_loss: 0.1508 - val_accuracy: 0.9742\n",
      "Epoch 14/15\n",
      "4699/4699 [==============================] - 613s 131ms/step - loss: 0.0160 - accuracy: 0.9932 - val_loss: 0.2254 - val_accuracy: 0.9713\n",
      "Epoch 15/15\n",
      "4690/4699 [============================>.] - ETA: 1s - loss: 0.0147 - accuracy: 0.9947"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=15,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('JS_CNN_0805_15Epoc.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tap test\n",
    "from pathlib import Path\n",
    "input_dir = Path(\"H:/DoAnTotNghiep/DAMH/data_crawl/tap_test_virus/res\")\n",
    "data_files = input_dir.glob('*')\n",
    "data_total = []\n",
    "label_total = []\n",
    "\n",
    "# them vao mang luu ten de tim mau nhan nham\n",
    "file_names = []\n",
    "\n",
    "a=0\n",
    "b=0\n",
    "i=0\n",
    "for file in data_files:\n",
    "    file_names.append(file)\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read().replace('\\x00','').lower()\n",
    "        data_total.append(data)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model da luu tu lan train truoc\n",
    "from keras.models import load_model\n",
    "model_load = load_model('JS_CNN_0805_15Epoc.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sample tu tap data\n",
    "X_sample = tokenizer.texts_to_sequences(data_total)\n",
    "X_sample = pad_sequences(X_sample, padding='post', maxlen=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = model_load.predict(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "b=0\n",
    "for x in range(len(y_sample)):\n",
    "    if y_sample[x][0]>y_sample[x][1]:\n",
    "        a+=1\n",
    "        print(x, file_names[x])\n",
    "    else:\n",
    "        b+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check mẫu nhận nhầm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "input_dir = Path('C:\\\\Users\\\\sontdc\\\\Desktop\\\\virus_nham')\n",
    "data_files = input_dir.glob('*')\n",
    "data_total = []\n",
    "label_total = []\n",
    "\n",
    "# them vao mang luu ten de tim mau nhan nham\n",
    "file_names = []\n",
    "\n",
    "a=0\n",
    "b=0\n",
    "i=0\n",
    "for file in data_files:\n",
    "    file_names.append(file)\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read().replace('\\x00','').lower()\n",
    "        data_total.append(data)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = tokenizer.texts_to_sequences(data_total)\n",
    "X_sample = pad_sequences(X_sample, padding='post', maxlen=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = model_load.predict(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "b=0\n",
    "for x in range(len(y_sample)):\n",
    "    if y_sample[x][0]>y_sample[x][1]:\n",
    "        a+=1\n",
    "#         print(x, file_names[x])\n",
    "    else:\n",
    "        b+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đồ thị hóa quá trình huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "%matplotlib inline\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
